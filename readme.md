# Multimodal Chatbot
A scalable backend for a multimodal chatbot, built with FastAPI, ChromaDB, Redis, and HuggingFace Embeddings, with support for text conversations and potential for audio processing via Whisper. This project demonstrates a robust architecture for handling user sessions, context-aware responses, and vector-based retrieval of chat history.

## Project Overview  
This repository contains the backend for a multimodal chatbot capable of handling text-based conversations with plans to extend to audio inputs. The system uses FastAPI for the API layer, Redis for session management, ChromaDB for vector storage and retrieval of chat history, HuggingFace Embeddings (all-MiniLM-L6-v2) for semantic search, and Llama-2 (via llama-cpp-python) for text generation. The architecture is designed to be scalable, with a focus on context-aware responses by leveraging vector embeddings.

### Features  
- **Text Chat**: Users can interact via a /chat/text endpoint, with responses generated by Llama-2.
- **Session Management**: Redis stores user sessions, ensuring continuity across interactions.
- **Context-Aware Responses**: ChromaDB stores chat history as embeddings, enabling retrieval of relevant past messages.
- **Semantic Search**: HuggingFace Embeddings power vector-based retrieval for better context.
- **Logging**: loguru provides detailed logging for debugging and performance monitoring.
- **Future-Ready**: Designed to support audio processing with Whisper and potential integration with Model Context Protocol (MCP).

## Prerequisites
- Python: 3.10+
- Redis: For session management (install locally or use a hosted service).
- ChromaDB: For vector storage (can run locally or via HTTP client).
- CUDA (optional): For GPU acceleration with Llama-2 (requires compatible NVIDIA GPU).

## Setup Instructions

1. Clone the repository
2. Go into cloned directory and then into the `/backend` directory
3. Create a virtual environment
    ```
    python -m venv .venv
    source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```
4. Install dependencies
    ```
     pip install -r requirements.txt
    ```
5. Set up the chroma DB and Redis, the docker compose file in the outer directory can be used to setup these.
    The Entire project will be dockerized soon.
6. Once the databases are setup run the following command:
   ```
   uvicorn backend.src.main:app --host 0.0.0.0 --port 8000 --reload
   ```
6. Visit the `http://localhost:8000/docs` to explore the API endpoints

[Screencast from 28-03-25 12_15_36 PM IST.mp4](docs/Screencast%20from%2028-03-25%2012_15_36%20PM%20IST.mp4)


## Future Improvements
1. GPU Support: Enable CUDA for faster inference on NVIDIA GPUs.
2. Multimodal Support: Fully integrate Whisper for audio processing.
3. Model Context Protocol (MCP): Add MCP to fetch real-time external data (e.g., weather APIs).
4. Reinforcement Learning: Implement RLHF to fine-tune responses based on user feedback.
5. Performance Optimization: Use torch.compile for faster inference and explore batch processing in ChromaDB.


## Acknowledgments
- Thanks to the open-source communities behind FastAPI, LangChain, ChromaDB, and HuggingFace for their amazing tools.
- Special shoutout to the xAI team for Grok, which assisted in debugging and architecture design.